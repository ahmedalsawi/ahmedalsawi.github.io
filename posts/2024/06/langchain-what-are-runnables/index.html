<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  Langchain - What are Runnables · Techiedeepdive
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="description" content="Runnable Interface  Link to heading   From langchain documentation, There are few abstractions provide consistent API for applications. For example, ChatModel takes list of string (or list of chat messages, PromptValue) and generates ChatMessage.
 Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool">
<meta name="keywords" content="">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="Langchain - What are Runnables"/>
<meta name="twitter:description" content="Runnable Interface  Link to heading   From langchain documentation, There are few abstractions provide consistent API for applications. For example, ChatModel takes list of string (or list of chat messages, PromptValue) and generates ChatMessage.
 Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool"/>

<meta property="og:title" content="Langchain - What are Runnables" />
<meta property="og:description" content="Runnable Interface  Link to heading   From langchain documentation, There are few abstractions provide consistent API for applications. For example, ChatModel takes list of string (or list of chat messages, PromptValue) and generates ChatMessage.
 Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/2024/06/langchain-what-are-runnables/" /><meta property="og:image" content=""/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-06-11T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-06-11T00:00:00+00:00" /><meta property="og:site_name" content="Techiedeepdive" />





<link rel="canonical" href="/posts/2024/06/langchain-what-are-runnables/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css" integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="">
      Techiedeepdive
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/reading-list/">Reading list</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/tags/">Tags</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="/posts/2024/06/langchain-what-are-runnables/">
              Langchain - What are Runnables
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-06-11T00:00:00Z">
                June 11, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              4-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/llm/">llm</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/langchain/">langchain</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <h1 id="runnable-interface">
  Runnable Interface
  <a class="heading-link" href="#runnable-interface">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>From langchain documentation, There are few abstractions provide consistent API for applications. For example, <code>ChatModel</code> takes list of string (or list of chat messages, PromptValue) and generates ChatMessage.</p>
<blockquote>
<p>Prompt Dictionary PromptValue
ChatModel Single string, list of chat messages or a PromptValue ChatMessage
LLM Single string, list of chat messages or a PromptValue String
OutputParser The output of an LLM or ChatModel Depends on the parser
Retriever Single string List of Documents
Tool Single string or dictionary, depending on the tool Depends on the tool</p>
</blockquote>
<p>The Runnable interface is langchain abstraction to create objects and chain them.</p>
<blockquote>
<p>Runnable interface
To make it as easy as possible to create custom chains, we&rsquo;ve implemented a &ldquo;Runnable&rdquo; protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about in this section.</p>
<p>This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:</p>
<p>stream: stream back chunks of the response
invoke: call the chain on an input
batch: call the chain on a list of inputs</p>
</blockquote>
<h1 id="ollama-deepdive">
  Ollama deepdive
  <a class="heading-link" href="#ollama-deepdive">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>The smallest example to run langchain can be done with <code>Ollama</code> as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">from</span> langchain_community.llms <span style="color:#fff;font-weight:bold">import</span> Ollama
<span style="color:#fff;font-weight:bold">input</span> = <span style="color:#fff;font-weight:bold">input</span>(<span style="color:#0ff;font-weight:bold">&#34;What is your question?&#34;</span>)
llm = Ollama(model=<span style="color:#0ff;font-weight:bold">&#34;llama2&#34;</span>)
res = llm.invoke(<span style="color:#fff;font-weight:bold">input</span>)
<span style="color:#fff;font-weight:bold">print</span> (res)
</code></pre></div><p>Let&rsquo;s start with <code>Ollama</code> LLM object used in the above example defined in <code>libs/community/langchain_community/llms/ollama.py</code></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">class</span> Ollama(BaseLLM, _OllamaCommon):
    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Ollama locally runs large language models.
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">    To use, follow the instructions at https://ollama.ai/.
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">    Example:
</span><span style="color:#0ff;font-weight:bold">        .. code-block:: python
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">            from langchain_community.llms import Ollama
</span><span style="color:#0ff;font-weight:bold">            ollama = Ollama(model=&#34;llama2&#34;)
</span></code></pre></div><p>Tracing <code>Ollama</code> class hierarchy all the way up to pudantic class (need to do more pydantic <em>cough cough</em>)</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">class</span> BaseLLM(BaseLanguageModel[<span style="color:#fff;font-weight:bold">str</span>], ABC):
    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Base LLM abstract interface.&#34;&#34;&#34;</span>

<span style="color:#fff;font-weight:bold">class</span> BaseLanguageModel(
    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
):

<span style="color:#fff;font-weight:bold">class</span> RunnableSerializable(Serializable, Runnable[Input, Output]):
    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Runnable that can be serialized to JSON.&#34;&#34;&#34;</span>

<span style="color:#fff;font-weight:bold">class</span> Serializable(BaseModel, ABC):
    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Serializable base class.&#34;&#34;&#34;</span>
</code></pre></div><p>Back to <code>libs/core/langchain_core/language_models/llms.py</code>, where <code>_OllamaCommon</code> defines some Ollama specific attributes.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">class</span> _OllamaCommon(BaseLanguageModel):
    base_url: <span style="color:#fff;font-weight:bold">str</span> = <span style="color:#0ff;font-weight:bold">&#34;http://localhost:11434&#34;</span>
    <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Base url the model is hosted under.&#34;&#34;&#34;</span>
</code></pre></div><p>So, Let&rsquo;s look at <code>invoke</code>. The Ollama LLM object gets it from <code>libs/core/langchain_core/language_models/llms.py</code>. Here is stack of function calls starting from <code>invoke</code></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#fff;font-weight:bold">def</span> invoke(
        self,
        <span style="color:#fff;font-weight:bold">input</span>: LanguageModelInput,
        config: Optional[RunnableConfig] = <span style="color:#fff;font-weight:bold">None</span>,
        *,
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        **kwargs: Any,
    ) -&gt; <span style="color:#fff;font-weight:bold">str</span>:
        config = ensure_config(config)
        <span style="color:#fff;font-weight:bold">return</span> (
            self.generate_prompt(
                [self._convert_input(<span style="color:#fff;font-weight:bold">input</span>)],
                stop=stop,
                callbacks=config.get(<span style="color:#0ff;font-weight:bold">&#34;callbacks&#34;</span>),
                tags=config.get(<span style="color:#0ff;font-weight:bold">&#34;tags&#34;</span>),
                metadata=config.get(<span style="color:#0ff;font-weight:bold">&#34;metadata&#34;</span>),
                run_name=config.get(<span style="color:#0ff;font-weight:bold">&#34;run_name&#34;</span>),
                run_id=config.pop(<span style="color:#0ff;font-weight:bold">&#34;run_id&#34;</span>, <span style="color:#fff;font-weight:bold">None</span>),
                **kwargs,
            )
            .generations[<span style="color:#ff0;font-weight:bold">0</span>][<span style="color:#ff0;font-weight:bold">0</span>]
            .text
        )


    <span style="color:#fff;font-weight:bold">def</span> generate_prompt(
        ...
    ) -&gt; LLMResult:
        prompt_strings = [p.to_string() <span style="color:#fff;font-weight:bold">for</span> p in prompts]
        <span style="color:#fff;font-weight:bold">return</span> self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)


    <span style="color:#fff;font-weight:bold">def</span> generate(
        ...
    )
    <span style="color:#fff;font-weight:bold">def</span> _generate_helper(
        self,
        prompts: List[<span style="color:#fff;font-weight:bold">str</span>],
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]],
        run_managers: List[CallbackManagerForLLMRun],
        new_arg_supported: <span style="color:#fff;font-weight:bold">bool</span>,
        **kwargs: Any,
    ) -&gt; LLMResult:
        <span style="color:#fff;font-weight:bold">try</span>:
            output = (
                self._generate(
                    prompts,
                    stop=stop,
                    <span style="color:#007f7f"># TODO: support multiple run managers</span>
                    run_manager=run_managers[<span style="color:#ff0;font-weight:bold">0</span>] <span style="color:#fff;font-weight:bold">if</span> run_managers <span style="color:#fff;font-weight:bold">else</span> <span style="color:#fff;font-weight:bold">None</span>,
                    **kwargs,
                )
                <span style="color:#fff;font-weight:bold">if</span> new_arg_supported
                <span style="color:#fff;font-weight:bold">else</span> self._generate(prompts, stop=stop)
</code></pre></div><p>And eventually <code>_generate</code> is called from Ollama which calls <code>_stream_with_aggregation</code></p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#fff;font-weight:bold">def</span> _generate(  <span style="color:#007f7f"># type: ignore[override]</span>
        self,
        prompts: List[<span style="color:#fff;font-weight:bold">str</span>],
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        images: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        run_manager: Optional[CallbackManagerForLLMRun] = <span style="color:#fff;font-weight:bold">None</span>,
        **kwargs: Any,
    ) -&gt; LLMResult:
        <span style="color:#0ff;font-weight:bold">&#34;&#34;&#34;Call out to Ollama&#39;s generate endpoint.
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">        Args:
</span><span style="color:#0ff;font-weight:bold">            prompt: The prompt to pass into the model.
</span><span style="color:#0ff;font-weight:bold">            stop: Optional list of stop words to use when generating.
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">        Returns:
</span><span style="color:#0ff;font-weight:bold">            The string generated by the model.
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">        Example:
</span><span style="color:#0ff;font-weight:bold">            .. code-block:: python
</span><span style="color:#0ff;font-weight:bold">
</span><span style="color:#0ff;font-weight:bold">                response = ollama(&#34;Tell me a joke.&#34;)
</span><span style="color:#0ff;font-weight:bold">        &#34;&#34;&#34;</span>
        <span style="color:#007f7f"># TODO: add caching here.</span>
        generations = []
        <span style="color:#fff;font-weight:bold">for</span> prompt in prompts:
            final_chunk = <span style="color:#fff;font-weight:bold">super</span>()._stream_with_aggregation(...
</code></pre></div><p>Starting with <code>_stream_with_aggregation</code> calling few functions until it calls the actual <code>post</code> on Ollama REST API.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#fff;font-weight:bold">def</span> _stream_with_aggregation(
        self,
        prompt: <span style="color:#fff;font-weight:bold">str</span>,
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        run_manager: Optional[CallbackManagerForLLMRun] = <span style="color:#fff;font-weight:bold">None</span>,
        verbose: <span style="color:#fff;font-weight:bold">bool</span> = <span style="color:#fff;font-weight:bold">False</span>,
        **kwargs: Any,
    ) -&gt; GenerationChunk:
        final_chunk: Optional[GenerationChunk] = <span style="color:#fff;font-weight:bold">None</span>
        <span style="color:#fff;font-weight:bold">for</span> stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#fff;font-weight:bold">def</span> _create_generate_stream(
        self,
        prompt: <span style="color:#fff;font-weight:bold">str</span>,
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        images: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        **kwargs: Any,
    ) -&gt; Iterator[<span style="color:#fff;font-weight:bold">str</span>]:
        payload = {<span style="color:#0ff;font-weight:bold">&#34;prompt&#34;</span>: prompt, <span style="color:#0ff;font-weight:bold">&#34;images&#34;</span>: images}
        <span style="color:#fff;font-weight:bold">yield from</span> self._create_stream(
            payload=payload,
            stop=stop,
            api_url=<span style="color:#0ff;font-weight:bold">f</span><span style="color:#0ff;font-weight:bold">&#34;</span><span style="color:#0ff;font-weight:bold">{</span>self.base_url<span style="color:#0ff;font-weight:bold">}</span><span style="color:#0ff;font-weight:bold">/api/generate&#34;</span>,
            **kwargs,
        )
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    <span style="color:#fff;font-weight:bold">def</span> _create_stream(
        self,
        api_url: <span style="color:#fff;font-weight:bold">str</span>,
        payload: Any,
        stop: Optional[List[<span style="color:#fff;font-weight:bold">str</span>]] = <span style="color:#fff;font-weight:bold">None</span>,
        **kwargs: Any,
    ) -&gt; Iterator[<span style="color:#fff;font-weight:bold">str</span>]:
        <span style="color:#fff;font-weight:bold">if</span> self.stop is not <span style="color:#fff;font-weight:bold">None</span> and stop is not <span style="color:#fff;font-weight:bold">None</span>:
            <span style="color:#fff;font-weight:bold">raise</span> ValueError(<span style="color:#0ff;font-weight:bold">&#34;`stop` found in both the input and default params.&#34;</span>)
        <span style="color:#fff;font-weight:bold">elif</span> self.stop is not <span style="color:#fff;font-weight:bold">None</span>:
            stop = self.stop

...
        response = requests.post(
            url=api_url,
            headers={
                <span style="color:#0ff;font-weight:bold">&#34;Content-Type&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;application/json&#34;</span>,
                **(self.headers <span style="color:#fff;font-weight:bold">if</span> <span style="color:#fff;font-weight:bold">isinstance</span>(self.headers, <span style="color:#fff;font-weight:bold">dict</span>) <span style="color:#fff;font-weight:bold">else</span> {}),
            },
            json=request_payload,
            stream=<span style="color:#fff;font-weight:bold">True</span>,
            timeout=self.timeout,
        )
</code></pre></div>
      </div>


      <footer>
        


        
        
        
        
        

        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2024
    
    ·
    
      Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA-4.0</a>
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.369d90111ae4409b4e51de5efd23a46b92663fcc82dc9a0efde7f70bffc3f949.js" integrity="sha256-Np2QERrkQJtOUd5e/SOka5JmP8yC3JoO/ef3C//D&#43;Uk="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
