<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  LiteLLM - One LLM proxy to rule them all · Techiedeepdive
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="description" content="The tag line for liteLLM is simple and awesome
 Call 100&#43; LLMs using the same Input/Output Format
 Hello world  Link to heading   This is small example from litellm docs using Ollama. I have Ollama running locally. So, that was easy.
from litellm import completion response = completion( model=&#34;ollama/llama2&#34;, messages=[{ &#34;content&#34;: &#34;respond in 20 words. who are you?&#34;,&#34;role&#34;: &#34;user&#34;}], api_base=&#34;http://localhost:11434&#34; ) print(response)  ModelResponse(id=&lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&rsquo;, choices=[Choices(finish_reason=&lsquo;stop&rsquo;, index=0, message=Message(content=&ldquo;I&rsquo;m just an AI assistant, here to help!">
<meta name="keywords" content="">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="LiteLLM - One LLM proxy to rule them all"/>
<meta name="twitter:description" content="The tag line for liteLLM is simple and awesome
 Call 100&#43; LLMs using the same Input/Output Format
 Hello world  Link to heading   This is small example from litellm docs using Ollama. I have Ollama running locally. So, that was easy.
from litellm import completion response = completion( model=&#34;ollama/llama2&#34;, messages=[{ &#34;content&#34;: &#34;respond in 20 words. who are you?&#34;,&#34;role&#34;: &#34;user&#34;}], api_base=&#34;http://localhost:11434&#34; ) print(response)  ModelResponse(id=&lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&rsquo;, choices=[Choices(finish_reason=&lsquo;stop&rsquo;, index=0, message=Message(content=&ldquo;I&rsquo;m just an AI assistant, here to help!"/>

<meta property="og:title" content="LiteLLM - One LLM proxy to rule them all" />
<meta property="og:description" content="The tag line for liteLLM is simple and awesome
 Call 100&#43; LLMs using the same Input/Output Format
 Hello world  Link to heading   This is small example from litellm docs using Ollama. I have Ollama running locally. So, that was easy.
from litellm import completion response = completion( model=&#34;ollama/llama2&#34;, messages=[{ &#34;content&#34;: &#34;respond in 20 words. who are you?&#34;,&#34;role&#34;: &#34;user&#34;}], api_base=&#34;http://localhost:11434&#34; ) print(response)  ModelResponse(id=&lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&rsquo;, choices=[Choices(finish_reason=&lsquo;stop&rsquo;, index=0, message=Message(content=&ldquo;I&rsquo;m just an AI assistant, here to help!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/2024/05/litellm-one-llm-proxy-to-rule-them-all/" /><meta property="og:image" content=""/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-25T00:00:00+00:00" /><meta property="og:site_name" content="Techiedeepdive" />





<link rel="canonical" href="/posts/2024/05/litellm-one-llm-proxy-to-rule-them-all/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css" integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="">
      Techiedeepdive
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/reading-list/">Reading list</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/tags/">Tags</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="/posts/2024/05/litellm-one-llm-proxy-to-rule-them-all/">
              LiteLLM - One LLM proxy to rule them all
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-05-25T00:00:00Z">
                May 25, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              2-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa-solid fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/llm/">llm</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/llamaindex/">llamaIndex</a>
    </span></div>

        </div>
      </header>

      <div class="post-content">
        
        <p>The tag line for liteLLM is simple and awesome</p>
<blockquote>
<p>Call 100+ LLMs using the same Input/Output Format</p>
</blockquote>
<h1 id="hello-world">
  Hello world
  <a class="heading-link" href="#hello-world">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>This is small example from litellm docs using Ollama. I have Ollama running locally. So, that was easy.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">from</span> litellm <span style="color:#fff;font-weight:bold">import</span> completion

response = completion(
    model=<span style="color:#0ff;font-weight:bold">&#34;ollama/llama2&#34;</span>,
    messages=[{ <span style="color:#0ff;font-weight:bold">&#34;content&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;respond in 20 words. who are you?&#34;</span>,<span style="color:#0ff;font-weight:bold">&#34;role&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;user&#34;</span>}],
    api_base=<span style="color:#0ff;font-weight:bold">&#34;http://localhost:11434&#34;</span>
)
<span style="color:#fff;font-weight:bold">print</span>(response)
</code></pre></div><blockquote>
<p>ModelResponse(id=&lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&rsquo;, choices=[Choices(finish_reason=&lsquo;stop&rsquo;, index=0, message=Message(content=&ldquo;I&rsquo;m just an AI assistant, here to help!&rdquo;, role=&lsquo;assistant&rsquo;))], created=1716627590, model=&lsquo;ollama/llama2&rsquo;, object=&lsquo;chat.completion&rsquo;, system_fingerprint=None, usage=Usage(prompt_tokens=31, completion_tokens=14, total_tokens=45))</p>
</blockquote>
<h1 id="python-sdk">
  Python SDK
  <a class="heading-link" href="#python-sdk">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>Well, fancy way of saying, they abstract API on top of difference LLM providers interface. Still, super helpful because OpenAI and hugggingface are using different names for their APIs.</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">from</span> litellm <span style="color:#fff;font-weight:bold">import</span> completion
<span style="color:#fff;font-weight:bold">import</span> os

<span style="color:#007f7f">## set ENV variables</span>
os.environ[<span style="color:#0ff;font-weight:bold">&#34;OPENAI_API_KEY&#34;</span>] = <span style="color:#0ff;font-weight:bold">&#34;your-api-key&#34;</span>

response = completion(
  model=<span style="color:#0ff;font-weight:bold">&#34;gpt-3.5-turbo&#34;</span>,
  messages=[{ <span style="color:#0ff;font-weight:bold">&#34;content&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;Hello, how are you?&#34;</span>,<span style="color:#0ff;font-weight:bold">&#34;role&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;user&#34;</span>}]
)
</code></pre></div><p>I tried it with local Ollama and it works!</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">from</span> litellm <span style="color:#fff;font-weight:bold">import</span> completion

response = completion(
    model=<span style="color:#0ff;font-weight:bold">&#34;ollama/llama2&#34;</span>,
    messages=[{ <span style="color:#0ff;font-weight:bold">&#34;content&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;respond in 20 words. who are you?&#34;</span>,<span style="color:#0ff;font-weight:bold">&#34;role&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;user&#34;</span>}],
    api_base=<span style="color:#0ff;font-weight:bold">&#34;http://localhost:11434&#34;</span>
)
<span style="color:#fff;font-weight:bold">print</span>(response)
</code></pre></div><h1 id="proxy">
  Proxy
  <a class="heading-link" href="#proxy">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h1>
<p>This is where things get interesting. litellm can start local server working as proxy. It provides these features:</p>
<ul>
<li>Hooks for auth</li>
<li>Hooks for logging</li>
<li>Cost tracking</li>
<li>Rate Limiting</li>
</ul>
<p>The most interesting part it can mock API for different providers. For example, you can use local model from a provider and use openAI library exactly like you have OpenAI access. Cool!</p>
<div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pip install <span style="color:#0ff;font-weight:bold">&#39;litellm[proxy]&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">litellm --model huggingface/bigcode/starcoder
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#fff;font-weight:bold">import</span> openai <span style="color:#007f7f"># openai v1.0.0+</span>
client = openai.OpenAI(api_key=<span style="color:#0ff;font-weight:bold">&#34;anything&#34;</span>,base_url=<span style="color:#0ff;font-weight:bold">&#34;http://0.0.0.0:4000&#34;</span>) <span style="color:#007f7f"># set proxy to base_url</span>

<span style="color:#007f7f"># request sent to model set on litellm proxy, `litellm --model`</span>

response = client.chat.completions.create(model=<span style="color:#0ff;font-weight:bold">&#34;gpt-3.5-turbo&#34;</span>, messages = [
{
<span style="color:#0ff;font-weight:bold">&#34;role&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;user&#34;</span>,
<span style="color:#0ff;font-weight:bold">&#34;content&#34;</span>: <span style="color:#0ff;font-weight:bold">&#34;this is a test request, write a short poem&#34;</span>
}
])

<span style="color:#fff;font-weight:bold">print</span>(response)
</code></pre></div><p>Bonus: They have cool dashboard to track things for your app. Again COOL!
It can be accessed <code>localhost:4000/ui</code> but it needs postgress database.</p>

      </div>


      <footer>
        


        
        
        
        
        

        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2024
    
    ·
    
      Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA-4.0</a>
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.369d90111ae4409b4e51de5efd23a46b92663fcc82dc9a0efde7f70bffc3f949.js" integrity="sha256-Np2QERrkQJtOUd5e/SOka5JmP8yC3JoO/ef3C//D&#43;Uk="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
