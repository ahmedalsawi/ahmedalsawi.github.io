<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on Techiedeepdive</title>
    <link>/tags/llm/</link>
    <description>Recent content in Llm on Techiedeepdive</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Thu, 27 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM - FAISS Hello world</title>
      <link>/posts/2025/02/llm-faiss-hello-world/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/02/llm-faiss-hello-world/</guid>
      <description>&lt;p&gt;These are my comments about FAISS vector indexing library from &lt;a href=&#34;https://medium.com/loopio-tech/how-to-use-faiss-to-build-your-first-similarity-search-bf0f708aa772&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;. The idea is simple really. FAISS is an index used for storing and search for vector embedding. AFAIK, it&amp;rsquo;s made for large scale applications. so, maybe that&amp;rsquo;s advantage over the llamaIndex vectorIndex storage. Maybe.&lt;/p&gt;&#xA;&lt;p&gt;First, we create vector encoding using SentenceTransformer. in this example, it&amp;rsquo;s using &lt;code&gt;paraphrase-mpnet-base-v2&lt;/code&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;encoder &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; SentenceTransformer(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;paraphrase-mpnet-base-v2&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vectors &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; encoder&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;encode(text)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then create index, and store these vector in FAISS index.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intel GPU One API drivers - A journey</title>
      <link>/posts/2025/01/intel-gpu-one-api-drivers-a-journey/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/01/intel-gpu-one-api-drivers-a-journey/</guid>
      <description>&lt;p&gt;This is all stated when i thought to try Ollama running on Intel iGPU on laptop. This took me into the rabbit hole and back.&lt;/p&gt;&#xA;&lt;h1 id=&#34;installation-and-running-in-gpu-mode&#34;&gt;&#xA;  Installation and running in GPU mode&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#installation-and-running-in-gpu-mode&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;I initially got &amp;ldquo;no GPU detected&amp;rdquo; from Ollama, So I had to dig deeper into the source. Based on the code, I needed to install &lt;code&gt;libze_intel_gpu.so&lt;/code&gt; from the following packages.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install libze-intel-gpu-dev libze-intel-gpu1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, enable Intel GPU env vars&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - AgentVerse</title>
      <link>/posts/2025/01/llm-agentverse/</link>
      <pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/01/llm-agentverse/</guid>
      <description>&lt;p&gt;&lt;code&gt;AgentVerse&lt;/code&gt; is yet another agent framework but this one is interesting because:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It has game-like UI. UI is implemented with &lt;code&gt;Phaser&lt;/code&gt; which is HTML game development framework(live and learn)&lt;/li&gt;&#xA;&lt;li&gt;This is very thing LLamaIndex agent I can find (see &lt;code&gt;ToolAgent&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;AgentVerse provides 3 top level classes and CLI programs:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Simulation CLI&lt;/li&gt;&#xA;&lt;li&gt;Simulation GUI&lt;/li&gt;&#xA;&lt;li&gt;Task Solving&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I will look deeper into task solving &lt;code&gt;TaskSolving&lt;/code&gt; because it looks the most relevant at the moment. Starting with &lt;code&gt;agentverse_command/main_tasksolving_cli.py&lt;/code&gt; which is really simple, just parsing task and task directory and creating &lt;code&gt;TaskSolving&lt;/code&gt; object and &lt;code&gt;run()&lt;/code&gt; it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Deep dive into AGENTS Framework</title>
      <link>/posts/2024/12/llm-deep-dive-into-agents-framework/</link>
      <pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/12/llm-deep-dive-into-agents-framework/</guid>
      <description>&lt;p&gt;There are several frameworks that support multi-agent communication. For example, autogen, crewai, or AGENTS. The problem here each framework implements its own infra for LLM and don&amp;rsquo;t play nice with llamaIndex. This is deep dive into how &lt;code&gt;agents&lt;/code&gt; framework works and how they design multi-agent env.&lt;/p&gt;&#xA;&lt;h1 id=&#34;initialization&#34;&gt;&#xA;  Initialization&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#initialization&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Starting with the code from examples directory where it calls &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;agents,sop,environment &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; init(args&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;agent)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prepare(agents, sop, environment)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;run(agents,sop,environment)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;init&lt;/code&gt; creates env, agents and SOP from config files. And connect them together.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ollama - Hello world</title>
      <link>/posts/2024/12/ollama-hello-world/</link>
      <pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/12/ollama-hello-world/</guid>
      <description>&lt;p&gt;This is a quick hello world to run local model with Ollama.&lt;/p&gt;&#xA;&lt;h1 id=&#34;ollama-docker&#34;&gt;&#xA;  Ollama docker&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#ollama-docker&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;The simplest way is running Ollama docker. To create the container, we just to fire up 2 commands.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker exec -it ollama ollama run llama2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You also can open a shell that container with&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker exec -it ollama bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Side note, Sometimes the containers can linger around, so we need to clean up the containers before restarting a new one.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Deep dive into openDevin</title>
      <link>/posts/2024/11/llm-deep-dive-into-opendevin/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/11/llm-deep-dive-into-opendevin/</guid>
      <description>&lt;h1 id=&#34;tldr&#34;&gt;&#xA;  TL;DR&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#tldr&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;This is a deconstruction of Opendevin user-to-agent development assistant framework.&lt;/p&gt;&#xA;&lt;p&gt;I will jump around a lot because I have no idea what is going on but this the TL;DR&lt;/p&gt;&#xA;&lt;p&gt;Opendevin uses the following abstraction to manage data from client to Agents and back using websocket and internal &lt;em&gt;steam&lt;/em&gt; and &lt;em&gt;subscribers&lt;/em&gt; to these streams.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SessionManager -&amp;gt; Session -&amp;gt; AgentSession -&amp;gt; AgentController -&amp;gt; Agent&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;starting-opendevin-with-ollama&#34;&gt;&#xA;  Starting opendevin with Ollama&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#starting-opendevin-with-ollama&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;The simplest way to run it using docker and Ollama running locally. Here is the command that worked for me&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
