<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm on Techiedeepdive</title>
    <link>/tags/llm/</link>
    <description>Recent content in llm on Techiedeepdive</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 11 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Langchain - What are Runnables</title>
      <link>/posts/2024/06/langchain-what-are-runnables/</link>
      <pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/06/langchain-what-are-runnables/</guid>
      <description>Runnable Interface  Link to heading   From langchain documentation, There are few abstractions provide consistent API for applications. For example, ChatModel takes list of string (or list of chat messages, PromptValue) and generates ChatMessage.
 Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool</description>
    </item>
    
    <item>
      <title>LLM - AgentVerse</title>
      <link>/posts/2024/06/llm-agentverse/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/06/llm-agentverse/</guid>
      <description>AgentVerse is yet another agent framework but this one is interesting because:
 It has game-like UI. UI is implemented with Phaser which is HTML game development framework(live and learn) This is very thing LLamaIndex agent I can find (see ToolAgent)  AgentVerse provides 3 top level classes and CLI programs:
 Simulation CLI Simulation GUI Task Solving  I will look deeper into task solving TaskSolving because it looks the most relevant at the moment.</description>
    </item>
    
    <item>
      <title>LiteLLM - One LLM proxy to rule them all</title>
      <link>/posts/2024/05/litellm-one-llm-proxy-to-rule-them-all/</link>
      <pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/05/litellm-one-llm-proxy-to-rule-them-all/</guid>
      <description>The tag line for liteLLM is simple and awesome
 Call 100+ LLMs using the same Input/Output Format
 Hello world  Link to heading   This is small example from litellm docs using Ollama. I have Ollama running locally. So, that was easy.
from litellm import completion response = completion( model=&amp;#34;ollama/llama2&amp;#34;, messages=[{ &amp;#34;content&amp;#34;: &amp;#34;respond in 20 words. who are you?&amp;#34;,&amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;}], api_base=&amp;#34;http://localhost:11434&amp;#34; ) print(response)  ModelResponse(id=&amp;lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&amp;rsquo;, choices=[Choices(finish_reason=&amp;lsquo;stop&amp;rsquo;, index=0, message=Message(content=&amp;ldquo;I&amp;rsquo;m just an AI assistant, here to help!</description>
    </item>
    
    <item>
      <title>LLM - Deep dive into openDevin</title>
      <link>/posts/2024/05/llm-deep-dive-into-opendevin/</link>
      <pubDate>Sat, 25 May 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/05/llm-deep-dive-into-opendevin/</guid>
      <description>TLDR  Link to heading   This is a deconstruction of Opendevin user-to-agent development assistant framework. I will jump around a lot because I have no idea what is going on.
Opendevin uses the following abstraction to manage data from client to Agents and back using websocket and internal steam and subscribers to these streams.
SessionManager -&amp;gt; Session -&amp;gt; AgentSession -&amp;gt; AgentController -&amp;gt; Agent Starting opendevin with Ollama  Link to heading   The simplest way to run it using docker and Ollama running locally.</description>
    </item>
    
    <item>
      <title>LLM - Deep dive into AGENTS Framework</title>
      <link>/posts/2024/05/llm-deep-dive-into-agents-framework/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/05/llm-deep-dive-into-agents-framework/</guid>
      <description>There are several frameworks that support multi-agent communication. For example, autogen, crewai, or AGENTS. The problem here each framework implements its own infra for LLM and don&amp;rsquo;t play nice with llamaIndex. This is deep dive into how agents framework works and how they design multi-agent env.
Initialization  Link to heading   Starting with the code from examples directory where it calls init and run.
agents,sop,environment = init(args.agent) prepare(agents, sop, environment) run(agents,sop,environment) init creates env, agents and SOP from config files.</description>
    </item>
    
    <item>
      <title>LLM - FAISS</title>
      <link>/posts/2024/04/llm-faiss/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/04/llm-faiss/</guid>
      <description>These are my comments about FAISS vector indexing library from post. The idea is simple really. FAISS is an index used for storing and search for vector embedding. AFAIK, it&amp;rsquo;s made for large scale applications. so, maybe that&amp;rsquo;s advantage over the llamaIndex vectorIndex storage. Maybe.
First, we create vector encoding using SentenceTransformer. in this example, it&amp;rsquo;s using paraphrase-mpnet-base-v2
encoder = SentenceTransformer(&amp;#34;paraphrase-mpnet-base-v2&amp;#34;) vectors = encoder.encode(text) Then create index, and store these vector in FAISS index.</description>
    </item>
    
    <item>
      <title>Ollama - Hello world</title>
      <link>/posts/2024/04/ollama-hello-world/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/04/ollama-hello-world/</guid>
      <description>This is a quick hello world to run local model with Ollama.
Ollama docker  Link to heading   The simplest way is running Ollama in docker. To create the container, we just to fire up 2 commands.
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama docker exec -it ollama ollama run llama2 You also can open a shell that container with
docker exec -it ollama bash Side note, Sometimes the containers can liger around, so we need to clean up the containers before restarting a new one.</description>
    </item>
    
    <item>
      <title>llamaindex - demystifying the magic</title>
      <link>/posts/2024/03/llamaindex-demystifying-the-magic/</link>
      <pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/03/llamaindex-demystifying-the-magic/</guid>
      <description>LlamaIndex is a python library for LLM applications. It provides several abstractions/utilities to make LLM RAG application easier. This is deepdive into the stages of Llamaindex and the source code.
Loading  Link to heading   Starting with LlamaIndex readers. I will dig deeper into SimpleDirectoryReader which, as name suggest, a simple reader but powerful enough to handle several file types.
Document  Link to heading   The docs have a simple example to load one document using SimpleDirectoryReader.</description>
    </item>
    
  </channel>
</rss>
