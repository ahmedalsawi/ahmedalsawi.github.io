<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on Techiedeepdive</title>
    <link>/tags/llm/</link>
    <description>Recent content in Llm on Techiedeepdive</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 17 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM - MCP Hello world</title>
      <link>/posts/2025/05/llm-mcp-hello-world/</link>
      <pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/05/llm-mcp-hello-world/</guid>
      <description>&lt;p&gt;From &lt;a href=&#34;https://modelcontextprotocol.io/introduction&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1&lt;/a&gt;, The one-liner for MCP as follows&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;MCP sounds like a good approach for extendible agentic tools&lt;/p&gt;&#xA;&lt;h1 id=&#34;first-example&#34;&gt;&#xA;  First example&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#first-example&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Step 1, First install &lt;code&gt;mcp&lt;/code&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install mcp&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Step 2, The server with a tool and resource&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# server.py&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;mcp.server.fastmcp&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; FastMCP&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Create an MCP server&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mcp &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; FastMCP(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Demo&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Add an addition tool&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@mcp.tool&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;add&lt;/span&gt;(a: int, b: int) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; int:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Add two numbers&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; a &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;+&lt;/span&gt; b&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b949e;font-style:italic&#34;&gt;# Add a dynamic greeting resource&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;@mcp.resource&lt;/span&gt;(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;greeting://&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{name}&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#d2a8ff;font-weight:bold&#34;&gt;get_greeting&lt;/span&gt;(name: str) &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;-&amp;gt;&lt;/span&gt; str:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Get a personalized greeting&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff7b72&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#79c0ff&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;Hello, &lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;{&lt;/span&gt;name&lt;span style=&#34;color:#a5d6ff&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;!&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Step 3, Run the dev server&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - llamaindex - demystifying the magic</title>
      <link>/posts/2025/05/llm-llamaindex-demystifying-the-magic/</link>
      <pubDate>Sat, 03 May 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/05/llm-llamaindex-demystifying-the-magic/</guid>
      <description>&lt;p&gt;LlamaIndex is a python library for LLM applications. It provides several abstractions/utilities to make LLM RAG application easier. This is deepdive into the stages of Llamaindex and the source code.&lt;/p&gt;&#xA;&lt;h2 id=&#34;loading&#34;&gt;&#xA;  Loading&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#loading&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Starting with LlamaIndex readers. I will dig deeper into &lt;code&gt;SimpleDirectoryReader&lt;/code&gt; which, as name suggest, a simple reader but powerful enough to handle several file types.&lt;/p&gt;&#xA;&lt;h3 id=&#34;document&#34;&gt;&#xA;  Document&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#document&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The docs have a simple example to load one document using &lt;code&gt;SimpleDirectoryReader&lt;/code&gt;. Note that reader returns &lt;code&gt;Document&lt;/code&gt; object (or array of them).&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Aider Hello world</title>
      <link>/posts/2025/04/llm-aider-hello-world/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/04/llm-aider-hello-world/</guid>
      <description>&lt;p&gt;This is a hello-world for &lt;code&gt;aider&lt;/code&gt; which is a nice tool to generate code with AI using command line&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Aider is AI pair programming in your terminal&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;install&#34;&gt;&#xA;  Install&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#install&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;The installation uses &lt;code&gt;uv&lt;/code&gt; which seems like a new python package manager. Cool! will circle back later.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python -m pip install aider-install&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;aider-install&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, we have aider in &lt;code&gt;PATH&lt;/code&gt; which will point &lt;code&gt;$HOME/.local/bin/aider&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - LiteLLM One LLM proxy to rule them all</title>
      <link>/posts/2025/03/llm-litellm-one-llm-proxy-to-rule-them-all/</link>
      <pubDate>Sat, 22 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/03/llm-litellm-one-llm-proxy-to-rule-them-all/</guid>
      <description>&lt;p&gt;The tagline for LiteLLM is simple and awesome&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Call 100+ LLMs using the same Input/Output Format&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;hello-world&#34;&gt;&#xA;  Hello world&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#hello-world&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;This is a small example from LiteLLM docs using Ollama. I have Ollama running locally, so that was easy.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff7b72&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;litellm&lt;/span&gt; &lt;span style=&#34;color:#ff7b72&#34;&gt;import&lt;/span&gt; completion&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;response &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; completion(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;ollama/llama2&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    messages&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;[{ &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;respond in 20 words. who are you?&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt; }],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    api_base&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;http://localhost:11434&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(response)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;ModelResponse(id=&amp;lsquo;chatcmpl-d1c86df4-5feb-419d-8e8a-fd876ad46085&amp;rsquo;, choices=[Choices(finish_reason=&amp;lsquo;stop&amp;rsquo;, index=0, message=Message(content=&amp;ldquo;I&amp;rsquo;m just an AI assistant, here to help!&amp;rdquo;, role=&amp;lsquo;assistant&amp;rsquo;))], created=1716627590, model=&amp;lsquo;ollama/llama2&amp;rsquo;, object=&amp;lsquo;chat.completion&amp;rsquo;, system_fingerprint=None, usage=Usage(prompt_tokens=31, completion_tokens=14, total_tokens=45))&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - What are Langchain Runnables</title>
      <link>/posts/2025/03/llm-what-are-langchain-runnables/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/03/llm-what-are-langchain-runnables/</guid>
      <description>&lt;h1 id=&#34;runnable-interface&#34;&gt;&#xA;  Runnable Interface&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#runnable-interface&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;From Langchain documentation, there are a few abstractions that provide a consistent API for applications. For example, &lt;code&gt;ChatModel&lt;/code&gt; takes a list of strings (or a list of chat messages, PromptValue) and generates ChatMessage.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Prompt Dictionary PromptValue&#xA;ChatModel Single string, list of chat messages or a PromptValue ChatMessage&#xA;LLM Single string, list of chat messages or a PromptValue String&#xA;OutputParser The output of an LLM or ChatModel Depends on the parser&#xA;Retriever Single string List of Documents&#xA;Tool Single string or dictionary, depending on the tool Depends on the tool&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - FAISS Hello world</title>
      <link>/posts/2025/02/llm-faiss-hello-world/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/02/llm-faiss-hello-world/</guid>
      <description>&lt;p&gt;These are my comments about FAISS vector indexing library from &lt;a href=&#34;https://medium.com/loopio-tech/how-to-use-faiss-to-build-your-first-similarity-search-bf0f708aa772&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;. The idea is simple really. FAISS is an index used for storing and search for vector embedding. AFAIK, it&amp;rsquo;s made for large scale applications. so, maybe that&amp;rsquo;s advantage over the llamaIndex vectorIndex storage. Maybe.&lt;/p&gt;&#xA;&lt;p&gt;First, we create vector encoding using SentenceTransformer. in this example, it&amp;rsquo;s using &lt;code&gt;paraphrase-mpnet-base-v2&lt;/code&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;encoder &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; SentenceTransformer(&lt;span style=&#34;color:#a5d6ff&#34;&gt;&amp;#34;paraphrase-mpnet-base-v2&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vectors &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; encoder&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;encode(text)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then create index, and store these vector in FAISS index.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intel GPU One API drivers - A journey</title>
      <link>/posts/2025/01/intel-gpu-one-api-drivers-a-journey/</link>
      <pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/01/intel-gpu-one-api-drivers-a-journey/</guid>
      <description>&lt;p&gt;This is all stated when i thought to try Ollama running on Intel iGPU on laptop. This took me into the rabbit hole and back.&lt;/p&gt;&#xA;&lt;h1 id=&#34;installation-and-running-in-gpu-mode&#34;&gt;&#xA;  Installation and running in GPU mode&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#installation-and-running-in-gpu-mode&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;I initially got &amp;ldquo;no GPU detected&amp;rdquo; from Ollama, So I had to dig deeper into the source. Based on the code, I needed to install &lt;code&gt;libze_intel_gpu.so&lt;/code&gt; from the following packages.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install libze-intel-gpu-dev libze-intel-gpu1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, enable Intel GPU env vars&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - AgentVerse</title>
      <link>/posts/2025/01/llm-agentverse/</link>
      <pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate>
      <guid>/posts/2025/01/llm-agentverse/</guid>
      <description>&lt;p&gt;&lt;code&gt;AgentVerse&lt;/code&gt; is yet another agent framework but this one is interesting because:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It has game-like UI. UI is implemented with &lt;code&gt;Phaser&lt;/code&gt; which is HTML game development framework(live and learn)&lt;/li&gt;&#xA;&lt;li&gt;This is very thing LLamaIndex agent I can find (see &lt;code&gt;ToolAgent&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;AgentVerse provides 3 top level classes and CLI programs:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Simulation CLI&lt;/li&gt;&#xA;&lt;li&gt;Simulation GUI&lt;/li&gt;&#xA;&lt;li&gt;Task Solving&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I will look deeper into task solving &lt;code&gt;TaskSolving&lt;/code&gt; because it looks the most relevant at the moment. Starting with &lt;code&gt;agentverse_command/main_tasksolving_cli.py&lt;/code&gt; which is really simple, just parsing task and task directory and creating &lt;code&gt;TaskSolving&lt;/code&gt; object and &lt;code&gt;run()&lt;/code&gt; it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Deep dive into AGENTS Framework</title>
      <link>/posts/2024/12/llm-deep-dive-into-agents-framework/</link>
      <pubDate>Sat, 14 Dec 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/12/llm-deep-dive-into-agents-framework/</guid>
      <description>&lt;p&gt;There are several frameworks that support multi-agent communication. For example, autogen, crewai, or AGENTS. The problem here each framework implements its own infra for LLM and don&amp;rsquo;t play nice with llamaIndex. This is deep dive into how &lt;code&gt;agents&lt;/code&gt; framework works and how they design multi-agent env.&lt;/p&gt;&#xA;&lt;h1 id=&#34;initialization&#34;&gt;&#xA;  Initialization&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#initialization&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Starting with the code from examples directory where it calls &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;agents,sop,environment &lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;=&lt;/span&gt; init(args&lt;span style=&#34;color:#ff7b72;font-weight:bold&#34;&gt;.&lt;/span&gt;agent)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prepare(agents, sop, environment)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;run(agents,sop,environment)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;init&lt;/code&gt; creates env, agents and SOP from config files. And connect them together.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Ollama - Hello world</title>
      <link>/posts/2024/12/llm-ollama-hello-world/</link>
      <pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/12/llm-ollama-hello-world/</guid>
      <description>&lt;p&gt;This is a quick hello world to run local model with Ollama.&lt;/p&gt;&#xA;&lt;h1 id=&#34;ollama-docker&#34;&gt;&#xA;  Ollama docker&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#ollama-docker&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;The simplest way is running Ollama docker. To create the container, we just to fire up 2 commands.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker exec -it ollama ollama run llama2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You also can open a shell that container with&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker exec -it ollama bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Side note, Sometimes the containers can linger around, so we need to clean up the containers before restarting a new one.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM - Deep dive into openDevin</title>
      <link>/posts/2024/11/llm-deep-dive-into-opendevin/</link>
      <pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate>
      <guid>/posts/2024/11/llm-deep-dive-into-opendevin/</guid>
      <description>&lt;h1 id=&#34;tldr&#34;&gt;&#xA;  TL;DR&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#tldr&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;This is a deconstruction of Opendevin user-to-agent development assistant framework.&lt;/p&gt;&#xA;&lt;p&gt;I will jump around a lot because I have no idea what is going on but this the TL;DR&lt;/p&gt;&#xA;&lt;p&gt;Opendevin uses the following abstraction to manage data from client to Agents and back using websocket and internal &lt;em&gt;steam&lt;/em&gt; and &lt;em&gt;subscribers&lt;/em&gt; to these streams.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SessionManager -&amp;gt; Session -&amp;gt; AgentSession -&amp;gt; AgentController -&amp;gt; Agent&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;starting-opendevin-with-ollama&#34;&gt;&#xA;  Starting opendevin with Ollama&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#starting-opendevin-with-ollama&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;The simplest way to run it using docker and Ollama running locally. Here is the command that worked for me&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
