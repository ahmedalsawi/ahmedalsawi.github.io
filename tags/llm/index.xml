<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm on Techiedeepdive</title>
    <link>/tags/llm/</link>
    <description>Recent content in llm on Techiedeepdive</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 05 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM - FAISS</title>
      <link>/posts/2024/04/llm-faiss/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/04/llm-faiss/</guid>
      <description>These are my comments about FAISS vector indexing library from post. The idea is simple really. FAISS is an index used for storing and search for vector embedding. AFAIK, it&amp;rsquo;s made for large scale applications. so, maybe that&amp;rsquo;s advantage over the llamaIndex vectorIndex storage. Maybe.
First, we create vector encoding using SentenceTransformer. in this example, it&amp;rsquo;s using paraphrase-mpnet-base-v2
encoder = SentenceTransformer(&amp;#34;paraphrase-mpnet-base-v2&amp;#34;) vectors = encoder.encode(text) Then create index, and store these vector in FAISS index.</description>
    </item>
    
    <item>
      <title>Ollama - Hello world</title>
      <link>/posts/2024/04/ollama-hello-world/</link>
      <pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/04/ollama-hello-world/</guid>
      <description>This is a quick hello world to run local model with Ollama.
Ollama docker  Link to heading   The simplest way is running Ollama in docker. To create the container, we just to fire up 2 commands.
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama docker exec -it ollama ollama run llama2 You also can open a shell that container with
docker exec -it ollama bash Side note, Sometimes the containers can liger around, so we need to clean up the containers before restarting a new one.</description>
    </item>
    
    <item>
      <title>llamaindex - demystifying the magic</title>
      <link>/posts/2024/03/llamaindex-demystifying-the-magic/</link>
      <pubDate>Sat, 16 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>/posts/2024/03/llamaindex-demystifying-the-magic/</guid>
      <description>LlamaIndex is a python library for LLM applications. It provides several abstractions/utilities to make LLM RAG application easier. This is deepdive into the stages of Llamaindex and the source code.
Loading  Link to heading   Starting with LlamaIndex readers. I will dig deeper into SimpleDirectoryReader which, as name suggest, a simple reader but powerful enough to handle several file types.
Document  Link to heading   The docs have a simple example to load one document using SimpleDirectoryReader.</description>
    </item>
    
  </channel>
</rss>
